{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy-based method to generate CLIP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from open_clip import create_model_and_transforms, tokenize\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from statistics import mean, stdev, median\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_classes = 3\n",
    "# Path to the parent directory containing the TF folders\n",
    "output_path = f\"../ImgData/supernovaRGBa_tf_class3\"\n",
    "\n",
    "# Load the CLIP model\n",
    "model, preprocess_train, preprocess_val = create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "top_k = 10 # Number of images to select by entropy\n",
    "\n",
    "class Entropy(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Computes the entropy of the input tensor:\n",
    "    $H(x) = sum_i (p_i log_2(p_i) )$ where $p_i$ is the input tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dim=None,\n",
    "                 keepdim: bool = False,\n",
    "                 normalize_input: bool = True,\n",
    "                 normalize_output: bool = True):\n",
    "        super().__init__()\n",
    "        self._dim = dim\n",
    "        self._keepdim = keepdim\n",
    "        self._normalize_input = normalize_input\n",
    "        self._normalize_output = normalize_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self._dim is None:\n",
    "            dim = tuple(range(len(x.shape)))\n",
    "        else:\n",
    "            dim = self._dim\n",
    "\n",
    "        if self._normalize_input:\n",
    "            scale = torch.sum(x, dim, keepdim=True)\n",
    "            p = x / scale\n",
    "        else:\n",
    "            p = x\n",
    "\n",
    "        entropy = -torch.nansum(p * torch.log2(p), dim, keepdim=self._keepdim)\n",
    "\n",
    "        if self._normalize_output:\n",
    "            N = np.prod([x.shape[i] for i in dim])\n",
    "            entropy = entropy / np.log2(N)\n",
    "        return entropy\n",
    "\n",
    "class EntropyLosses(torch.nn.Module):\n",
    "    def __init__(self, opacity_weight=1):\n",
    "        super().__init__()\n",
    "        self._opacity_entropy = Entropy(dim=(1, 2), normalize_input=True, normalize_output=True)\n",
    "        self._opacity_weight = opacity_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        losses = torch.zeros(x.shape[0], dtype=x.dtype, device=x.device)\n",
    "\n",
    "        if self._opacity_weight > 0:\n",
    "            opacity = x[:, :, :, 3]\n",
    "            opacity_entropy = self._opacity_entropy(opacity)\n",
    "            losses = losses + self._opacity_weight * opacity_entropy\n",
    "\n",
    "        return losses\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def extract_frame_number(file_path):\n",
    "    match = re.search(r'r_(\\d+)\\.png', file_path)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Get the best view out of top k frames\n",
    "def get_best_frame(top_frames):\n",
    "    frame_numbers = [extract_frame_number(frame) for frame in top_frames if extract_frame_number(frame) is not None]\n",
    "    print(f\"Top frame numbers: {frame_numbers}\")\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean_frame = mean(frame_numbers)\n",
    "    std_dev = stdev(frame_numbers)\n",
    "\n",
    "    # Exclude outliers (those beyond 1 standard deviations from the mean)\n",
    "    filtered_frames = [frame for frame in frame_numbers if abs(frame - mean_frame) <= 1 * std_dev]\n",
    "    print(f\"Filtered frame numbers: {filtered_frames}\")\n",
    "\n",
    "    # Calculate the average of the filtered frame numbers\n",
    "    average_frame_number = int(mean(filtered_frames))\n",
    "    return average_frame_number, filtered_frames\n",
    "\n",
    "# Compute entropy for all frames and select the top 10 by entropy\n",
    "entropy_loss = EntropyLosses()\n",
    "\n",
    "for folder_name in os.listdir(output_path):\n",
    "    if folder_name.startswith(\"TF\"):\n",
    "        folder_path = os.path.join(output_path, folder_name, \"train\")\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing folder: {folder_name}\")\n",
    "        image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".png\")]\n",
    "\n",
    "        if len(image_files) < top_k:\n",
    "            raise ValueError(f\"Not enough images in {folder_path} to select {top_k}. Skipping.\")\n",
    "\n",
    "        entropy_scores = []\n",
    "        for img_path in tqdm(image_files):\n",
    "            img = Image.open(img_path).convert(\"RGBA\")\n",
    "            img_tensor = torch.from_numpy(np.array(img)).float().unsqueeze(0) / 255.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                entropy = entropy_loss(img_tensor).item()\n",
    "            entropy_scores.append((img_path, entropy))\n",
    "\n",
    "        # Sort by entropy and select top k frames\n",
    "        entropy_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_frames = [path for path, _ in entropy_scores[:top_k]]\n",
    "        best_frame_number, filtered_frame_number = get_best_frame(top_frames)\n",
    "        print(f\"Best frame number: {best_frame_number}\")\n",
    "        with open(os.path.join(output_path, folder_name, \"best_frames.txt\"), \"w\") as f:\n",
    "            f.write(f\"{best_frame_number}\\n\")\n",
    "            f.write(\"\\n\".join([str(frame) for frame in filtered_frame_number if frame != best_frame_number]))   \n",
    "        best_frame_path = os.path.join(folder_path, f\"r_{best_frame_number:04}.png\")\n",
    "        filtered_frames = [os.path.join(folder_path, f\"r_{frame:04}.png\") for frame in filtered_frame_number]\n",
    "\n",
    "        # Save unfilted and filtered embeddings\n",
    "        embeddings = []\n",
    "        for img_path in top_frames:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_tensor = preprocess_train(img).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embedding = model.encode_image(img_tensor).squeeze(0).numpy()\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        combined_embedding = np.mean(embeddings, axis=0)\n",
    "        embedding_path = os.path.join(output_path, folder_name, \"image_embedding_entropy.npy\")\n",
    "        np.save(embedding_path, combined_embedding)\n",
    "\n",
    "        embeddings = []\n",
    "        for img_path in filtered_frames:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_tensor = preprocess_train(img).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embedding = model.encode_image(img_tensor).squeeze(0).numpy()\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        combined_embedding = np.mean(embeddings, axis=0)\n",
    "        embedding_path = os.path.join(output_path, folder_name, \"image_filtered_embedding_entropy.npy\")\n",
    "        np.save(embedding_path, combined_embedding)\n",
    "\n",
    "\n",
    "        img = Image.open(best_frame_path).convert(\"RGB\")\n",
    "        img_tensor = preprocess_train(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            top1_embedding = model.encode_image(img_tensor).squeeze(0).numpy()\n",
    "        embedding_path = os.path.join(output_path, folder_name, \"image_top1_embedding_entropy.npy\")\n",
    "        np.save(embedding_path, top1_embedding)\n",
    "        print(f\"Saved embedding to {embedding_path}\")\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine text embedding with image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from open_clip import create_model_and_transforms, tokenize\n",
    "import torch\n",
    "\n",
    "# Path to the parent directory containing the TF folders\n",
    "output_path = f\"../ImgData/supernovaRGBa_tf_class3\"\n",
    "top_k = 10  # Number of images to select randomly from each folder\n",
    "image_embedding_name = \"image_embedding_entropy.npy\" # Image embedding file name used to combine with text embedding\n",
    "\n",
    "# TF to class mapping\n",
    "tf_to_class = {\n",
    "    \"TF00\": \"expanding shockwave\",\n",
    "    \"TF01\": \"turbulent plasma\",\n",
    "    \"TF02\": \"dense core ejecta\"\n",
    "}\n",
    "\n",
    "\n",
    "# Load the CLIP model\n",
    "model, preprocess_train, preprocess_val = create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Generates a CLIP embedding for the given text.\"\"\"\n",
    "    tokenized_text = tokenize([text])\n",
    "    with torch.no_grad():\n",
    "        text_embedding = model.encode_text(tokenized_text).squeeze(0).numpy()\n",
    "    return text_embedding\n",
    "\n",
    "def process_folder(folder_path, class_text, image_embedding_name=None):\n",
    "    \"\"\"Processes a folder to generate a combined CLIP embedding for images and text.\"\"\"\n",
    "    if image_embedding_name is None:\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.endswith(\".png\")]\n",
    "        if len(image_files) < top_k:\n",
    "            print(f\"Not enough images in {folder_path} to select {top_k}. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        # Select random images\n",
    "        selected_images = random.sample(image_files, top_k)\n",
    "        image_embeddings = []\n",
    "\n",
    "        for img_file in selected_images:\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_tensor = preprocess_train(img).unsqueeze(0)  # Preprocess and add batch dimension\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embedding = model.encode_image(img_tensor).squeeze(0).numpy()\n",
    "                image_embeddings.append(embedding)\n",
    "\n",
    "        # Average the image embeddings\n",
    "        combined_image_embedding = np.mean(image_embeddings, axis=0)\n",
    "    else:\n",
    "        # Load the image embedding\n",
    "        image_embedding_path = os.path.join(os.path.dirname(folder_path), image_embedding_name)\n",
    "        combined_image_embedding = np.load(image_embedding_path)\n",
    "\n",
    "    # Generate the text embedding\n",
    "    text_embedding = get_text_embedding(class_text)\n",
    "\n",
    "    # Combine image and text embeddings with equal weights\n",
    "    combined_embedding = (combined_image_embedding + text_embedding) / 2\n",
    "    return combined_embedding\n",
    "\n",
    "# Process each folder starting with \"TF\"\n",
    "for folder_name in os.listdir(output_path):\n",
    "    if folder_name.startswith(\"TF\"):\n",
    "        folder_path = os.path.join(output_path, folder_name, \"train\")\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        class_text = tf_to_class.get(folder_name, \"\")\n",
    "        print(f\"Processing folder: {folder_name}, Class text: {class_text}\")\n",
    "        combined_embedding = process_folder(folder_path, class_text, image_embedding_name)\n",
    "\n",
    "        if combined_embedding is not None:\n",
    "            # Save the combined embedding as a .npy file in the \"TF\" folder\n",
    "            embedding_path = os.path.join(output_path, folder_name, f\"{image_embedding_name.split('.')[0]}_plus_text.npy\")\n",
    "            np.save(embedding_path, combined_embedding)\n",
    "            print(f\"Saved combined embedding to {embedding_path}\")\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_class = {\n",
    "    \"TF00\": \"expanding shockwave (green), illustrates lower-density, cooler material expanding outward at extremely high velocities, representing the supernova's shockwave interacting with surrounding interstellar gas.\",\n",
    "    \"TF01\": \"turbulent plasma (blue), represent intermediate-density gases, likely consisting of heated stellar material, freshly fused elements created by the supernova itself, and plasma undergoing turbulent mixing.\",\n",
    "    \"TF02\": \"dense core ejecta (red), indicate dense, hotter core regions and heavier elements synthesized during the explosion. These areas are marked by intricate, filamentary structures caused by powerful shockwaves and fluid instabilities.\"\n",
    "}\n",
    "# Extract dataset name\n",
    "dataset_name = os.path.basename(output_path).split(\"RGBa\")[0]\n",
    "\n",
    "# Write information to a .txt file\n",
    "output_file = os.path.join(output_path, \"dataset_info.txt\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(f\"Dataset Name: {dataset_name}\\n This is a dataset of a supernova, which is an enormous stellar explosion that occurs at the end of a massive star's lifecycle, briefly outshining entire galaxies.\\n\")\n",
    "    f.write(\"Classes:\\n\")\n",
    "    for key, value in tf_to_class.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(f\"Dataset information written to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nli4volvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
